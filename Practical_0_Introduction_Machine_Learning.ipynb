{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Indaba-X-Kigali/Indaba-X-Kigali/blob/master/Practical_0_Introduction_Machine_Learning.ipynb\" \n",
    "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVR7i2f5X3lN"
   },
   "source": [
    "# Introduction to Machine Learning (ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkkD6MqvX3lQ"
   },
   "source": [
    "### What is Machine learning ?  \n",
    "\n",
    "We have seen Machine Learning as a buzzword for the past few years, the reason for this might be the high amount of data production by applications, the increase of computation power in the past few years and the development of better algorithms.\n",
    "\n",
    "Machine Learning is used anywhere from automating mundane tasks to offering intelligent insights, industries in every sector try to benefit from it. You may already be using a device that utilizes it. For example, a wearable fitness tracker like Fitbit, or an intelligent home assistant like Google Home. But there are much more examples of ML in use.\n",
    "\n",
    "#### Machine Learning defined #### \n",
    "According to Arthur Samuel, Machine Learning algorithms enable the computers to learn from data, and even improve themselves, without being explicitly programmed.\n",
    "\n",
    "Machine learning (ML) is a category of an algorithm that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. The basic premise of machine learning is to build algorithms that can receive input data and use statistical analysis to predict an output while updating outputs as new data becomes available.\n",
    "\n",
    "#### Prediction  —\n",
    "Machine learning can also be used in the prediction systems. Considering the loan example, to compute the probability of a fault, the system will need to classify the available data in groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaJqxH0-X3lS"
   },
   "source": [
    "## Motivation ### \n",
    "### Why do we need Machine Learning ? \n",
    "\n",
    "Machine Learning is a field which is raised out of Artificial Intelligence(AI). Applying AI, we wanted to build better and intelligent machines. But except for few mere tasks such as finding the shortest path between point A and B, we were unable to program more complex and constantly evolving challenges.There was a realisation that the only way to be able to achieve this task was to let machine learn from itself. This sounds similar to a child learning from its self. So machine learning was developed as a new capability for computers. And now machine learning is present in so many segments of technology, that we don’t even realise it while using it.\n",
    "\n",
    "Finding patterns in data on planet earth is possible only for human brains. The data being very massive, the time taken to compute is increased, and this is where Machine Learning comes into action, to help people with large data in minimum time.\n",
    "\n",
    "If big data and cloud computing are gaining importance for their contributions, machine learning as technology helps analyse those big chunks of data, easing the task of data scientists in an automated process and gaining equal importance and recognition.\n",
    "\n",
    "The techniques we use for data mining have been around for many years, but they were not effective as they did not have the competitive power to run the algorithms. If you run deep learning with access to better data, the output we get will lead to dramatic breakthroughs which is machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wcwx34wHX3lT"
   },
   "source": [
    "## Concept of Machine Learning explained\n",
    "\n",
    "Suppose you have information on the records of houses in specific areas, different towns in Kigali and you want to predict prices of the houses given the description; this can be handled a Machine Learning problem. By feeding the ML algorithm this input data, the program learns the various mappings from the input to output to make predictions on unseen input data(generalization).\n",
    "\n",
    "*Machine learning is the use computational methods to “learn” information directly from data.*\n",
    "\n",
    "![](https://www.codeproject.com/KB/AI/1146582/overall.PNG)\n",
    "\n",
    "The output is a learned model which is basically a function that describes for example what the price of a house is given information about the house. This works only in the case of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Jo5ktviX3lV"
   },
   "source": [
    "## How to solve a problem as machine learning problem ?  ##\n",
    "\n",
    "In the machine learning pradaigm, we need to find a mapping from an information or(data) to another. \n",
    "\n",
    "For example, we need to solve the following:\n",
    "##### prediction problems\n",
    "The rainfall in a city. (given a date, what is the predicted rainfall at that date).\n",
    "\n",
    "The stock prices. (given a date, what is the predction for the stocks for that date).\n",
    "\n",
    "The winner of a game. (given the historical data for gameplay, we need to find whose more likely to win the game. \n",
    "\n",
    "#### Modern Applications inclue: \n",
    "Spam filtering (given this email, is it a spam email or not). \n",
    "\n",
    "Facial Recognition given an image, does it contain a face or not.\n",
    "\n",
    "Speech processing, given an audio, convert it to text.\n",
    "\n",
    "Machine translation, given a text written in French, translate it to English, Arabic or Swahili. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xILxK9qPX3lW"
   },
   "source": [
    "In all the above examples we have:\n",
    "input information (x)\n",
    "output information (y)\n",
    "\n",
    "and x gets transformed to y. With a transformation (mapping) y = F(x)\n",
    "x and y , can take any values, continous or discrete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jG66SVPpX3lY"
   },
   "source": [
    "### So what qualifies as a machine learning problem ?\n",
    "\n",
    "#### In general any problem that is a mapping from input to output conditioned on the fact that we have a lot of examples i.e. pairs of (x,y) is a machine learning problem. This is only a subset of machine learning problems as we're going to see later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcwktTrMX3lZ"
   },
   "source": [
    "## Steps of solving a problem of machine learning. \n",
    "\n",
    "### Step -1 \" Goal formlation\" \n",
    "We need to predict something. I.e. given our inputs we need to find the appropriate ouput. \n",
    "\n",
    "### Step -2 \" Data collection\"\n",
    "The data could be sensor readins, or any information that we get from the real world. It can take any form, but some forms are computationally more convienient. \n",
    "\n",
    "data collection is corner stone in machine learning, we need to provide a lot of examples, i.e. pairs of (x,y) so that we can learn the relationship that will take our Xs and transform them into Ys. \n",
    "- usually in this part we also do data exploration, where we visualize the data. \n",
    "\n",
    "### Step -3 \"Selecting a family of models\" \n",
    "\n",
    "A model is what transforms my input to my output, it's a mathematical transformation (function). \n",
    "This function should be parameterized (composed of parameters). We can think of the parameters as the building blocks of my function, for example if we consider a polynomial, the parameters are the factors of that polynomial). $ y= f(x)= a_{0} + a_{1}x + a_{2}x^{2}$ the parameters in this case are $a_{0},a_{1}$ and $ a_{2}$ \n",
    "each value of these parameters determine a completely different function, The set of all the possible functions (usually infinite) are the class of functions or model class. Although this set is infinite, it's still can capture all the possible transfomrations, for example we can't describe a 5th order polynomial with the above function. \n",
    "\n",
    "This part usually comes after we visualize our data, and we can get from the visualization an insight about how the family of models look like. \n",
    "\n",
    "### Step -4 \" Finding the Best parameters\" \n",
    "In this step we search for the best parameters, \n",
    "so what does \"best\" means ? The answer is, the best parameters are the parameters that give us the most accurate predictions for both the given data, and for the unseen future data that we will encounter. \n",
    "\n",
    "As we can see, machine learning has transformed the problem of finding predicitons to an optimization problem which we hopefully know how to solve (think about continous optimization from calculus). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXhAzKSCX3lb"
   },
   "source": [
    "### Machine Learning Techniques\n",
    "\n",
    "Basically, we can group the machine learning techniques as a supervised learning or unsupervised learning problem. The difference is in supervised learning there are target variables  or output whereas  in unsupervised there are no target variables; the case study will be a supervised learning problem if we have information on the prices of the houses(target variables)\n",
    "\n",
    "![](https://fr.mathworks.com/help/stats/machinelearning_supervisedunsupervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hr_hUws3X3lc"
   },
   "source": [
    "## Learning: Representation, Evaluation and Optimization\n",
    "\n",
    "There exists three major components of every ML algorithm:\n",
    "\n",
    "**Representation**: how to represent knowledge from the input data. This is where we specify the class of function approximators we wish to use for the learner. Examples include decision trees, neural networks, support vector machines, linear models and the others.\n",
    "\n",
    "**Evaluation**: the way to evaluate possible models(hypotheses). Models differ by the set of parameters defined on them. Practically  models are evaluated on held out sets called test set. Examples include accuracy, prediction and recall, squared error and likelihood.\n",
    "\n",
    "**Optimization**: the search processes used in identifying parameters of the model that gives the highest performance. A popular example is gradient descent which is a convex optimization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdTMF3DrX3ld"
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/2400/1*oU3LAye3LxFcHg0UePmbSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uGOgGs9X3le"
   },
   "source": [
    "### Why do we learn: Generalization?\n",
    "\n",
    "The purpose of Machine learning is to learn representations from the data in a model that generalizes well to unseen data: we should be able to predict new prices from any house data set given a learned ML model. Practically, we divide the data into train, validation and test sets. The train set is what we use to learn the model, we then evaluate performance of the learned model on the test set. <br>*A good model neither overfits nor underfits the data (must fit well)*. If the model considers limited functions that does not learn the mapping well enough, it suffers from underfitting. If instead the model considers functions that learns more or less the mapping of every data point, it overfits(memorises) and either of these models generalises poorly (makes a lot of bad predictions) on unseen data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLrlQEckX3lf"
   },
   "source": [
    "## Now let's practically see how every step of machine learning is carried out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdz915bjX3lf"
   },
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtEKBdIOX3lg"
   },
   "outputs": [],
   "source": [
    "from   matplotlib               import pyplot              as plt\n",
    "from   matplotlib.colors        import ListedColormap\n",
    "from   scipy                    import optimize\n",
    "from   sklearn                  import datasets\n",
    "from   sklearn.model_selection  import train_test_split\n",
    "from   sklearn.linear_model     import LogisticRegression\n",
    "from   sklearn.preprocessing    import StandardScaler\n",
    "from   sklearn.metrics          import confusion_matrix, classification_report, log_loss\n",
    "import numpy.random             as rn\n",
    "import numpy                    as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7OowGwtQdru"
   },
   "source": [
    "## Task 1: Linear Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10i7Lnf6X3lo"
   },
   "source": [
    "### This comment illustaries the difference in the classical approach to solve the problem, and the machine learning approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eN_xpR2Qdr1"
   },
   "source": [
    "## Step -2  \"Data Collection\" \n",
    "\n",
    "let's assume that we have collected the following datapoints in Mars:\n",
    "\n",
    "\n",
    "{(10.0 , 44.0)\n",
    "\n",
    "(14.7 , 58.4)\n",
    "\n",
    "(19.5 , 71.6)\n",
    "\n",
    "(24.2 , 88.9)\n",
    "\n",
    "(28.9 , 112.6)\n",
    "\n",
    "(33.7 , 128.2)\n",
    "\n",
    "(38.4 , 136.9)\n",
    "\n",
    "(43.2 , 175.1)\n",
    "\n",
    "(47.9 , 180.9)\n",
    "\n",
    "(52.6 , 205.9)\n",
    "\n",
    "(57.4 , 219.2)\n",
    "\n",
    "(62.1 , 236.1)\n",
    "\n",
    "(66.8 , 255.7)\n",
    "\n",
    "(71.6 , 256.5)\n",
    "\n",
    "(76.3 , 280.6)\n",
    "\n",
    "(81.1 , 296.4)\n",
    "\n",
    "(85.8 , 336.4)\n",
    "\n",
    "(90.5 , 338.5)\n",
    "\n",
    "(95.3 , 353.9)\n",
    "\n",
    "(100.0 , 357.5)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UCFQafNQdr4"
   },
   "source": [
    "Now let's find a model that solves the problem: \n",
    "we will start by storing the datapoints in a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6NRYbspQdr7"
   },
   "outputs": [],
   "source": [
    "data = np.matrix([[ 10. ,  52.5],\n",
    " [ 14.7  ,63.4],\n",
    " [ 19.5  ,89.2],\n",
    " [ 24.2  ,78.2],\n",
    " [ 28.9 ,106.6],\n",
    " [ 33.7 ,126.9],\n",
    " [ 38.4 ,138. ],\n",
    " [ 43.2 ,148.3],\n",
    " [ 47.9 ,169.4],\n",
    " [ 52.6 ,191.4],\n",
    " [ 57.4 ,201.4],\n",
    " [ 62.1 ,239.4],\n",
    " [ 66.8 ,247.4],\n",
    " [ 71.6 ,255.8],\n",
    " [ 76.3 ,284.6],\n",
    " [ 81.1 ,306.8],\n",
    " [ 85.8 ,322.5],\n",
    " [ 90.5 ,330.3],\n",
    " [ 95.3 ,340.3],\n",
    " [100.  ,386. ]])\n",
    "\n",
    "m = data[:,0]\n",
    "F = data[:,1]\n",
    "m = np.array(m.flat)\n",
    "F = np.array(F.flat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L34w3ISPQdsD"
   },
   "source": [
    "Let's see what the datapoints look like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2387,
     "status": "ok",
     "timestamp": 1555106878541,
     "user": {
      "displayName": "Kobby Panford-Quainoo",
      "photoUrl": "",
      "userId": "01465049156779546079"
     },
     "user_tz": -120
    },
    "id": "ypez1J5bQdsF",
    "outputId": "1016096d-ae49-4587-d85b-6755d7a877c4"
   },
   "outputs": [],
   "source": [
    "plt.scatter(m.reshape(20),F.reshape(20),marker='x', color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLxo3MJSQdr0"
   },
   "source": [
    "\n",
    "## Step -3 \" Choosing a Family of Models\" \n",
    "\n",
    "Observe the data, it looks like a line, so our predictor(model) is probably going to be a line that goes throught the points. \n",
    "\n",
    "Therfore, we will build a parametrized model for the force, i.e. $F=\\theta_{0}+\\theta_{1}m$ where m is the mass. \n",
    "Then we will learn the values of theta. \n",
    "How to do that ? we will use data examples collected on Mars, and will use Machine learning algorithms to find the theta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMwEni2lX3l7"
   },
   "source": [
    "## Step -4  \" Finding the best parameters\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5YMYAs8QdsN"
   },
   "source": [
    "The algorithm we will use to solve this problem is known as: Linear regression. \n",
    "We will try to fit a linear model for the data above. \n",
    "how to do that? \n",
    "\n",
    "in the linear regression setting we define our function to be: \n",
    "$F=\\theta_{0}+\\theta_{1}m$ we can rewrite this equation in vector product form as $F=\\theta^{T}.M$ because it's faster for the computer to solve it. \n",
    "\n",
    "The goal is to find this $\\theta$ values that we substitute in the first equation to find the value of the force. How do we find the values of theta ? \n",
    "There are two techniques used:\n",
    "\n",
    "1- Using matrix inversion operation using the fomula \n",
    "$\\theta = (M^{T}×M)^{-1}×(M^{T}×F)$ where M is the matrix that contains all the values of $m_{i}$ mass of object i. We will augment the matrix with vector of 1s to make be able to learn free parameter $\\theta_{0}$ (bias in the F axis). F is the vector of all the forces $F_{i}$ (Force experienced on $mass_{i}$). \n",
    "\n",
    "2- Using gradient descent Algorithm. we'll talk about it later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pg5KKCKZQdsR"
   },
   "outputs": [],
   "source": [
    "M = np.zeros((20,2))\n",
    "M[:,0] = 1\n",
    "M[:,1] = m\n",
    "F = F.reshape(-1,1)\n",
    "\n",
    "theta = np.linalg.inv(M.T @ M)@ M.T@F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWh4mWupQdsW"
   },
   "source": [
    "Scientists have found the $GVc$ on Mars to be equal to 3.71. We should expect the value of $\\theta_{1}$ to be close to 3.7 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2357,
     "status": "ok",
     "timestamp": 1555106878551,
     "user": {
      "displayName": "Kobby Panford-Quainoo",
      "photoUrl": "",
      "userId": "01465049156779546079"
     },
     "user_tz": -120
    },
    "id": "Jq4PqRHJQdsX",
    "outputId": "8123fd62-52c6-45f4-ce8a-682c82834ade"
   },
   "outputs": [],
   "source": [
    "print(theta[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2341,
     "status": "ok",
     "timestamp": 1555106878553,
     "user": {
      "displayName": "Kobby Panford-Quainoo",
      "photoUrl": "",
      "userId": "01465049156779546079"
     },
     "user_tz": -120
    },
    "id": "emxc7TQ8Qdsg",
    "outputId": "1d4cc91c-914b-4597-ab69-1b294d10a938"
   },
   "outputs": [],
   "source": [
    "F_pred = M @ theta\n",
    "\n",
    "plt.scatter(m.reshape(20),F.reshape(20),marker='x', color=\"k\")\n",
    "plt.plot(m,F_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvKfwM1eQdsp"
   },
   "source": [
    "Notice the inversion of $(M^{T}×M)^{-1}$ is computationally expensive when the number of elements in M is big. The inversion is of order $O(n^{3})$. \n",
    "\n",
    "So we will use a faster algorithm known as gradient descent. \n",
    "\n",
    "in gradient descent, we find the optimal parameter by iteratively updating its value. The updates should minimize some objective function.\n",
    "In this case, our objective function is going to be the squared loss between my predections and the true values. \n",
    "\n",
    "$L = (F_{true} - F_{pred})^{2}$ \n",
    "\n",
    "in the gradient descent frame work we update the parameters of our model as :\n",
    "\n",
    "$\\theta_{new} = \\theta_{old} - \\alpha × \\nabla L(\\theta_{old}) $\n",
    "\n",
    "where $\\nabla L(\\theta)$ is the value of the gradient of L at $\\theta$ \n",
    "\n",
    "we know that $y_{pred} = M × \\theta$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do gradient descent works ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buFaTpZnQdsr"
   },
   "source": [
    "![Gradient Descent](https://raw.githubusercontent.com/Indaba-X-Kigali/Indaba-X-Kigali/master/figures/grad.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2659,
     "status": "ok",
     "timestamp": 1555106878896,
     "user": {
      "displayName": "Kobby Panford-Quainoo",
      "photoUrl": "",
      "userId": "01465049156779546079"
     },
     "user_tz": -120
    },
    "id": "tuUVczogQdsv",
    "outputId": "e33a46de-8709-4aa5-dcd5-f43edb9dfd86"
   },
   "outputs": [],
   "source": [
    "# implementation for the gradient descent for our example \n",
    "\n",
    "theta_old = np.zeros((2,1))\n",
    "lr = 1e-5\n",
    "\n",
    "def grad(theta):   \n",
    "    F_pred = M@theta\n",
    "    grad = -M.T@(F-F_pred)\n",
    "    return grad \n",
    "\n",
    "for i in range(100000):\n",
    "    \n",
    "    theta_new = theta_old - lr*grad(theta_old)\n",
    "    theta_old = theta_new\n",
    "\n",
    "print(theta_new[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhft120qQds0"
   },
   "source": [
    "See How the learned values converged to the analytic solution ... \n",
    "We call the process of the iterative parameter update the training process.\n",
    "\n",
    "now we can use this $\\theta$ to give us predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yav0aVaoX3mT"
   },
   "source": [
    "### Task 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j52vHPbUQdtA"
   },
   "source": [
    "#### Classification as a supervised learning problem\n",
    "\n",
    "##### What is classification?\n",
    "Classification is the problem of assigning a class $C_k$ from K distint classes to some input data $x$ where $k = 1, 2, \\cdots, K$. In classification tasks, we assume each input data can only belong to one class. As such, the input space $X$ is divided into decision regions which each data belonging to only one category by a decision function say $h$ which we call the classifier. When we have 2 classes then it is a binary classification task, more than one class we can have the multiclass setting.\n",
    "\n",
    "Let $ \\mathcal{D} = {(\\boldsymbol{x}_i, y_i)}_{i=1}^{N}$, where $y_i \\in {C_1, \\cdots, C_k} $ be some data set. The task of classification seeks to learn a function $ h_{\\boldsymbol \\theta} : \\boldsymbol{x} \\longrightarrow y $ such that the loss $ \\mathcal l(y, y^{\\prime}) $ is minimized (smallest number of misclassified). Lets us conisder the case of logistic regression as a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkMYp4H-QdtC"
   },
   "source": [
    "#### Logistic Regression.\n",
    "\n",
    "In this setting we learn the parameters of the function $ h_{\\boldsymbol \\theta} : \\boldsymbol{x} \\longrightarrow y $ such that the objective function given by $ \\mathcal{L}(y, y_{\\prime}) = \\frac{1}{N} \\sum_{n = 1}^{N} \\mathcal l(y, y^{\\prime})$ is minimized under the parameters $\\boldsymbol \\theta $, where $ y_{\\prime} = h(\\boldsymbol{x})$. By minimizing this loss, we learn a model that separates the classes of some new data set without labels with a high accuracy.\n",
    "\n",
    "In a binary setting we first compute the z-score given by $z = \\boldsymbol \\theta^{T} \\boldsymbol{x} + b $  and assign probablities to  data point by finding the sigmoid of the scores. Given some threshold (usually $0.5$), we then assign a class $y^{\\prime}$ to the data point if the probability is higher than this threshold or otherwise. The sigmoid is defined mathematically by $$  \\sigma (z) = \\dfrac{1}{1 + \\exp (-z)} = p(y = 1) $$ \n",
    "\n",
    "![](https://brohrer.github.io/images/what_nns_learn/drawing_160.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLlp6EBBYp3a"
   },
   "source": [
    "In a multiclass setting, we have the z-score as $z = \\boldsymbol \\theta^{T} \\boldsymbol{X} + \\boldsymbol{b} $. This computes the score of the data point belonging to each of the $K$ classes. We then apply the softmax fucntion to transform these scores into probabilities and the class with the highest probability is assigned to the data point. Here the softmax function is \n",
    "$$\\text{softmax} (\\boldsymbol{z}) = \\dfrac{\\exp (z)}{\\sum_{z^{\\prime}} \\exp (z^{\\prime})} = \\dfrac{\\exp (z - m)}{\\sum_{z^{\\prime}} \\exp (z^{\\prime} - m)}$$ where $ m = \\max \\boldsymbol z$\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/906/1*670CdxchunD-yAuUWdI7Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NajY2VVAQduF"
   },
   "source": [
    "We consider the [iris flower dataset](https://archive.ics.uci.edu/ml/datasets/iris) and learn a classifier for this data set using logistic regression\n",
    "\n",
    "![flower](https://archive.ics.uci.edu/ml/assets/MLimages/Large53.jpg)\n",
    "\n",
    "The dataset consists of 150 data points where each data point is a feature vector $\\boldsymbol x \\in \\mathbb{R}^4$ describing the attribute of a flower in the dataset, the four dimensions represent \n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "\n",
    "and the corresponding target $y \\in \\mathbb{Z}$ describes the class of the flower. It uses the integers $0$, $1$ and $2$ to represent the 3 classes of flowers in this dataset.\n",
    "\n",
    "0. Iris Setosa\n",
    "1. Iris Versicolour \n",
    "2. Iris Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mj0T7viUQduG"
   },
   "source": [
    "To begin with, we first load the data set and observe a plot of sepal length againt sepal with to visualize the separation between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1555910269073,
     "user": {
      "displayName": "Abigail Annkah",
      "photoUrl": "",
      "userId": "06235240296957457701"
     },
     "user_tz": -330
    },
    "id": "fsIDcmMQQduL",
    "outputId": "89e753fa-92d4-4f60-a2a0-8f380f3ede15"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "X , y= iris.data, iris.target\n",
    "\n",
    "print(\"The data set has\", X.shape[0], \"elements each of\", X.shape[-1], \"dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2223,
     "status": "ok",
     "timestamp": 1555910272324,
     "user": {
      "displayName": "Abigail Annkah",
      "photoUrl": "",
      "userId": "06235240296957457701"
     },
     "user_tz": -330
    },
    "id": "N-1ImxdGQduO",
    "outputId": "9de4795b-35a6-4507-b70e-a5c82094844d"
   },
   "outputs": [],
   "source": [
    "# Plotting the dataset\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000',  '#00FF00', '#0000FF'])\n",
    "\n",
    "K = 3\n",
    "x = X[-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for i, iris_class in enumerate(['Iris Setosa', 'Iris Versicolour', 'Iris Virginica']):\n",
    "    idx = y==i\n",
    "    ax.scatter(X[idx,0], X[idx,1], \n",
    "               c=cmap_bold.colors[i], edgecolor='k', \n",
    "               s=20, label=iris_class);\n",
    "ax.set(xlabel='sepal length (cm)', ylabel='sepal width (cm)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsPGE4QBQduV"
   },
   "source": [
    "Using in built tools from scikit learn, we fit a multinomial logistic regression on the training data set and evaluate the models, performance on the test data set. What is the purpose of splitting the data set before training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySBE-Kr-QduW"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:,:2], y, test_size = 0.20) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_jQh6ynd2xp"
   },
   "outputs": [],
   "source": [
    "def run_model(model, algname, targetnames, X_train, X_test, y_train, y_test):\n",
    "    # build the model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for training data\n",
    "    ypred = model.predict(X_train)\n",
    "    ypred_test = model.predict(X_test)\n",
    "    \n",
    "    print('--------------------------')\n",
    "    print(\"Loss on training: \", np.mean(ypred == y_train))\n",
    "    print(\"Loss on test: \", np.mean(ypred_test == y_test))\n",
    "    print('--------------------------')\n",
    "    print(\"Confusion_matrix\")\n",
    "    print(confusion_matrix(y_test, ypred_test))\n",
    "    print('--------------------------')\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test, ypred_test, target_names=targetnames))\n",
    "    print(' ')\n",
    "\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WN7G5YMQdud"
   },
   "source": [
    "Lets us run the model and observe the difference in training loss and test loss. What do you observe? Is this surprising? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2799,
     "status": "ok",
     "timestamp": 1555910894568,
     "user": {
      "displayName": "Abigail Annkah",
      "photoUrl": "",
      "userId": "06235240296957457701"
     },
     "user_tz": -330
    },
    "id": "pOFZNUK8Qdue",
    "outputId": "098d3138-2dde-4794-8921-fbf9e8757698"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "algname = \"Logistic regression\"\n",
    "targetnames= np.array(iris.target_names)\n",
    "ypred = run_model(logreg, algname, targetnames, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bI0uvW4GDtGZ"
   },
   "source": [
    "### Task 3: Principal Component Analysis (PCA)\n",
    "\n",
    "Thus far, we have looked at supervised learning tasks. We quickly consider PCA which is an unsupervised learning algorithm.  This algorithm finds application in data visualisation, dimentionality reduction, data compression and feature extraction. Given a high dimensional data, we believe that we can find a low-dimensional representation ( a small set of vectors) that explains the data well enough. Optimal projection onto this low-dimensional linear space (principal subspace) either maximizes the variance or minimizes projection cost (square reconstruction error). \n",
    "\n",
    "Using the MNIST handwritten digits for instance, which is 784 dimensional , it is most probably easier for humans to recognise each of these digits with fewer relevant dimensions that the original. And since humans can particularly visualise data up to 3-dimensions, we can find a low dimensional representation for the data. \n",
    "\n",
    "This can be done in 4 steps:\n",
    "\n",
    "\n",
    "*   **Mean subtraction** : compute mean and subtract from each datapoint to center data.<br>\n",
    " 1. $\\mu  = \\frac{1}{N} \\sum_{i=1}^{N}x_i $ <br>\n",
    " 2. $\\bar{x} = x - \\mu$\n",
    "\n",
    "\n",
    "*   **Compute covariance matrix** : Compute standard deviation for every dimension and divide each datapoint by the standard deviation. This keeps variance at 1 along each axis.<br>\n",
    "   $\\sigma  = \\frac{1}{N} \\sum_{i=1}^{N}(\\bar{x}_i^T\\bar{x}_i )^T$ = $\\frac{1}{N} \\sum_{i=1}^{N}\\bar{x}_i\\bar{x}_i^T $\n",
    "\n",
    "\n",
    "*  **Eigendecomposition of covariance matrix** : Compute eigenvalues and corresponding eigenvectors of the covariance matrix. Eigenvalues should have a D x D dimension.<br>\n",
    "$\\nu^T\\sigma\\nu = \\lambda$\n",
    "\n",
    "* **Projection** : Now we project each datapoint onto the principal subspace using the larger eigenvalues and rejecting the least eigenvectors. <br> The new low-dim representation $(\\hat{x}) $ of  each datapoint $x$  is given by\n",
    "$\\hat{x} = \\lambda^T(x - \\mu) $\n",
    "\n",
    "In essence, PCA algorithm centers the data by subtracting the mean. It then selects the direction with maximum variation as the first axis (principal) and finds another axis orthogonal to the first and covers as much variation as possible. This iterates until no possible axis is left. The axes with the least variability is removed without affecting the variability in the data and those that contributes significantly are used for reconstruction of our new low-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgauaUk5_vcn"
   },
   "source": [
    "Lets try this on the MNIST data. We will reduce the 784 dimensions to 2. You may  also try retain any number of dimensions say 3, 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O916M84Jel7F"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "MNIST, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "NUM_DATAPOINTS = 1000\n",
    "x = (MNIST.reshape(-1, 28 * 28)[:NUM_DATAPOINTS]) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3tSjQ33EHD7"
   },
   "outputs": [],
   "source": [
    "def do_pca(X ,num_components=0):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "      X : [N X D] - N inputs with dimensions D\n",
    "      num_components: n scalar \n",
    "    Return\n",
    "      X_hat: [N X M] - Reconstructed low dimensional N input with M components\n",
    "      evals: [1 X n] - eigenvalues selected with num_components\n",
    "      evecs: [N X n] - eigenvectors of corresponding eigenvalues\n",
    "    \"\"\"\n",
    "    \n",
    "    mu = np.mean(X,axis=0)                  # Compute mean of data\n",
    "    \n",
    "    x_bar = X - mu                          # Mean subtraction                          \n",
    "    sigma = np.cov(x_bar.T)                 # Compute covariance of X\n",
    "    evals, evecs = np.linalg.eigh(sigma)    # Compute eigenvalues and eigenvectors\n",
    "    indices = np.argsort(evals)             # Sort eigenvalues into indices\n",
    "    indices = indices[::-1]                 # Sort indices in descending order\n",
    "    evecs = evecs[:,indices]            \n",
    "    evals = evals[indices]\n",
    "    \n",
    "    if num_components > 0:\n",
    "        evecs = evecs[:,:num_components]    # Select eigenvectors with large variation \n",
    "        evals = evals[:num_components]      # Select corresponding eigenvalues\n",
    "\n",
    "    x_hat = (evecs.T @ x_bar.T).T\n",
    "    \n",
    "    return x_hat, evals, evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1555117653564,
     "user": {
      "displayName": "Kobby Panford-Quainoo",
      "photoUrl": "",
      "userId": "01465049156779546079"
     },
     "user_tz": -120
    },
    "id": "dcwWcjjn5Vje",
    "outputId": "69164df0-0739-4c6e-bc2b-8d4cc2886d8f"
   },
   "outputs": [],
   "source": [
    "x_hat, evals, evecs = do_pca(x, num_components=2)\n",
    "print(f\"Largest eigenvalues: {evals} \\n\\nFirst 20 Components: \\n{x_hat[:20]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1555117667276,
     "user": {
      "displayName": "Kobby Panford-Quainoo",
      "photoUrl": "",
      "userId": "01465049156779546079"
     },
     "user_tz": -120
    },
    "id": "RhdlG9pMerqD",
    "outputId": "6981b035-477e-4266-9570-bb7c6e06b227"
   },
   "outputs": [],
   "source": [
    "# Now lets compare our results with the Sklearn's implementation of the PCA Algorithm\n",
    "spca = PCA(n_components = 2, svd_solver = 'full')\n",
    "s_xhat = spca.fit_transform(x)\n",
    "s_evals = spca.explained_variance_\n",
    "print(f\"Largest eigenvalues: {s_evals} \\n\\nFirst 20 Components \\n{s_xhat[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGSAtBUVjldK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro_to_Machine_Learning_edited.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
