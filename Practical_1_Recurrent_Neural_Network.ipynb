{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Indaba-X-Kigali/Indaba-X-Kigali/blob/master/Practical_1_Recurrent_Neural_Network.ipynb\" \n",
    "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLb3FfKrX6WD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Ao7A0uxtAL7d",
    "outputId": "9dd75276-aae4-487d-af80-323fd5e76ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "QD6_TWPtTDVp",
    "outputId": "2e3e1e37-4dcf-4e7f-9628-1b10b3be02e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 4.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.9.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfFm3Ey4DbUS"
   },
   "source": [
    "# Part 0 : Introduction to RNN and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u8tCEHquXsM"
   },
   "source": [
    "RNN:\n",
    "\n",
    "![RNN3](https://user-images.githubusercontent.com/45560956/56459590-88e8af80-638d-11e9-9d27-e033f1044535.png)\n",
    "\n",
    "One on the main drawback of FFNN is their lack of ability to manage sequential data since for this kind of model, the features are invariant par permutation . RNN address this issue by using a memory cell $h_t$ that is modified every time it sees a new input $x_t$ in order to produce a time-dependent output $y_t$. For instance, for an input sequence $(x_0,x_1,x_2)$, instead of having $y = FFNN( [x_0,x_1,x_2] )$, The RNN will behave as follow :\n",
    "\n",
    "$y_1, h_1 = RNN(x_0)$ ;\n",
    "\n",
    "$y_2, h_2 = RNN(x_1)$ ;\n",
    "\n",
    "$y_3, h_3 = RNN(x_2)$ etc...\n",
    "\n",
    "The $x_t$ can be words in a sentence for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G634PFDbuMj8"
   },
   "source": [
    "\n",
    "  \n",
    "Mathematically, recurrent neural net can be written as:  \n",
    "\n",
    "$H_t = \\phi(U*X_t + W*H_{t-1})$\n",
    "\n",
    "$y_t = \\phi(V*H_{t})$\n",
    "\n",
    "U: weight matrix associated to the $x_t$\n",
    "\n",
    "W:  weight matrix associated to the  hidden layer\n",
    "\n",
    "V: weight matrix associated to the output layer\n",
    "\n",
    "The hidden state at time step t is $h_t$. It is a function of the input at the same time step $x_t$ and  the hidden state of the previous time step $ h_{t-1}$ The function  $\\phi$ can be the sigmoid function($\\sigma$) or tangent hyperbolic($tanh$)\n",
    "\n",
    "$y_t$ represente the output at time step t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovVcvPDQvhk1"
   },
   "source": [
    "**Intuitively**, we as humans processes sequences in a similar way:\n",
    "\n",
    "1)At each time step $t$, we update our memory (what we know) $h_t$ based on what we previously knew $h_{t-1}$ and the new information received $x_t$. Therefore, we can write $h_t = f(x_t, h_{t-1})$\n",
    "\n",
    "2) The information that we output may not have the same structure / size as what is inside our memory : we transform the information in our memory to produce a desired output : $y_t = g(h_t)$\n",
    "\n",
    "In RNN, for $f$ and $g$, we simply apply affine transformations to the inputs $x_t$ : $(U*X_t$), $h_{t-1}$ : ($W*H_{t-1}$) and $h_t$  : ($V*H_{t}$)to interpret them in a \"convenient way\" (w.r.t the task) and to make sure that dimensions are aligned before passing them to an activation function ($tanh$ or sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3TW3OMTxvsD"
   },
   "source": [
    "###  Limitations of RNN:\n",
    "\n",
    "During back propagation, recurrent neural networks suffer from the vanishing  gradient problem which means when we will update the model's parameters, the gradient will be very close to 0.\n",
    "\n",
    "more about backpropagation and vanishing gradient: http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/,  https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3mwI-Fyx-lv"
   },
   "source": [
    "### GRU\n",
    "\n",
    "GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. \n",
    "\n",
    "To solve that problem , GRU uses, so called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output.\n",
    "\n",
    "![GRU1](https://user-images.githubusercontent.com/45560956/56459570-458e4100-638d-11e9-95f1-450bd0693442.png)\n",
    "\n",
    "## Update Gate:\n",
    "\n",
    "$z_t = \\sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$\n",
    "\n",
    "$x_t$:  current input\n",
    "\n",
    "$W^{(z)}$ : weight associated to $x_t$\n",
    "\n",
    "$h_{t-1}$ : output of the previous unit\n",
    "\n",
    "$U^{(z)}$: weight associated to $h_{t-1}$\n",
    "\n",
    "The update gate helps the model to determine how much of the past information  needs to be passed along to the future. \n",
    "Suppose we have: \n",
    "### 'Hello, my name is Jean  and I'm from France. I speak....'\n",
    "\n",
    "To predict the next word we don't need to remember all the previous part. We can just use 'I'm from France' to now that the word to predict is 'French'. That's exactly what the update gate does, it allow the network to jus remember the usefull part to predict the next output.\n",
    "\n",
    "That allows the model to decide to copy all the information from the past and eliminate the risk of vanishing gradient problem\n",
    "\n",
    "## Reset Gate:\n",
    "\n",
    "$r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$\n",
    "\n",
    "this gate is used from the model to decide how much of the past information to forget. The difference  between Update Gate and Reset Gate comes in the weights and the gate’s usage.\n",
    "\n",
    "Coming back to the example above the reset gate tell to the model you don't need to remember  'Hello, my name is Jean  and' to predict the next word.\n",
    "\n",
    "## Current memory content\n",
    "\n",
    "This will use the reset gate to store the relevant information from the past.\n",
    "\n",
    "$h_{t}^{\\prime} = \\tanh(Wx_t +r_t ⊙  Uh_{t-1})$ \n",
    "\n",
    "$r_t ⊙  Uh_{t-1} $:  Hadamard (element-wise) product between the reset gate $r_t$ and $Uh_{t-1}$. That will determine what to remove from the previous time steps\n",
    "\n",
    "$\\tanh$: nonlinear activation function\n",
    "\n",
    "## Final memory at current time step:\n",
    "\n",
    "In this step the update gate is used.  It determines what to collect from the current memory content  $h_{t}^{\\prime}$ and what from the previous steps  $ h_{t-1}$\n",
    "\n",
    "$h_t = z_t  ⊙ h_{t-1} + (1-z_t)  ⊙ h_{t}^{\\prime}$\n",
    "\n",
    "* Apply element-wise multiplication to the update gate $z_t$ and $h_{t-1}$.\n",
    "* Apply element-wise multiplication to $(1-z_t) $and $h_{t}^{\\prime}$.\n",
    "* Sum the two results \n",
    "\n",
    "\n",
    "Putting all in a graph GRU looks like the schema below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyThQ4kXyJIe"
   },
   "source": [
    "\n",
    "more details [towardsdatascience](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be), [medium](https://medium.com/ai-journal/lstm-gru-recurrent-neural-networks-81fe2bcdf1f9), [karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "more references:\n",
    "\n",
    "https://skymind.ai/wiki/lstm\n",
    "\n",
    "https://medium.com/@carynmccarthy15/a-beginners-guide-to-recurrent-neural-networks-bfacb27bddb6\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "https://towardsdatascience.com/what-is-a-recurrent-nns-and-gated-recurrent-unit-grus-ea71d2a05a69\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQjdaHwbMn3G"
   },
   "source": [
    "# Part 1 : Question Answering with ParlAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqgtkAD_yZPC"
   },
   "source": [
    "# Introduction to Question Answering \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Search engines like Google and Yahoo! that allow the users to search for documents on the World\n",
    "Wide Web. In Search engines the user has to check each and every document to get useful answer to the\n",
    "question and it is a time consuming process. **The Question Answering (QA) system** reduces the search time toget exact answer to the question. Question Answering system is an important research area in information\n",
    "retrieval. Research on the area of Question Answering system started in the year 1960 and present lot of\n",
    "Question Answering systems have been developed. Question Answering system combines the research from\n",
    "different domains like Natural Language Processing, Artificial Intelligence, Information Retrieval and\n",
    "Information extraction.\n",
    "\n",
    "<img width=\"516\" alt=\"features-lexicalized-1\" src=\"https://user-images.githubusercontent.com/45560956/56459440-b16faa00-638b-11e9-9296-948b5a0ffbdc.png\">\n",
    "\n",
    "**The objective** of question answering system is to find exact answer to the question asked\n",
    "by user in natural language\n",
    "\n",
    "To understand the Question Answering subject, we firstly define the associated terms. \n",
    "\n",
    "*   **A Question Phrase** is the part of the question that says what is searched.\n",
    "*   ** Question Type**  refers to a categorization of the question for its purpose.\n",
    "*   ** Answer Type** refers to a class of objects which are sought by the question. \n",
    "*    **Question Focus** is the property or entity being searched by the question. \n",
    "*   **Question Topic** is the object or event that the question is about. \n",
    "*  **Candidate Passage** can broadly be defined as anything from a sentence to a document retrieved by a search engine in response to a question. \n",
    "* ** Candidate Answer** is the text ranked according to its suitability to as an answer\n",
    "\n",
    "\n",
    "But here we are not going to check all this details, in this case we just care about the  passage, question and answer because our data set contain a passages and  each passage is following by question and answer.\n",
    "![fa4_fig01](https://user-images.githubusercontent.com/45560956/56459472-188d5e80-638c-11e9-9cc4-1b2f35ea7ad4.jpg)\n",
    "\n",
    "Generally, the question answering system can be\n",
    "classified into two domains:\n",
    "*  **Closed domain question answering system**\n",
    "\n",
    " It deals with questions under a specific\n",
    "domain and can be seen as an easier task because NLP\n",
    "systems can exploit domain-specific knowledge frequently\n",
    "formalized in ontologies.  Alternatively, closed-domain might\n",
    "refer to a situation where only a limited type of questions are\n",
    "accepted, such as questions asking for descriptive rather\n",
    "than procedural information.\n",
    "* ** Open domain question answering system** \n",
    "\n",
    " It is deals with questions about nearly anything, and\n",
    "can only rely on general ontologies and world knowledge.\n",
    "\n",
    "Closed domain question answering system give more exact and correct answer than the open domain question answering system.\n",
    "\n",
    "# Exampe of Q/A:\n",
    "\n",
    "These examples are from the dataset that will use throughout this tutorial :  **Babi tasks**\n",
    "\n",
    "* Task 3 : Three supporting facts . Given a story, the model has to answer a question where only 3 sentences are useful (w.r.t the question)\n",
    "\n",
    "**Task 3: Three Supporting Facts **  \n",
    "John picked up the apple.  \n",
    "John went to the office.  \n",
    "John went to the kitchen.  \n",
    "John dropped the apple.   \n",
    "Where was the apple before the kitchen?  \n",
    "**Answer: office**\n",
    "\n",
    "* Task 2 : Two supporting facts . Given a story, the model has to answer a question where only 2 sentences are useful (w.r.t the question)\n",
    "\n",
    "**Task 2: Two Supporting Facts**  \n",
    "John is in the playground.  \n",
    "John picked up the football.  \n",
    "Bob went to the kitchen.  \n",
    "Where is the football?  \n",
    "**Answer: playground**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "1. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=11&cad=rja&uact=8&ved=2ahUKEwit9rWHls_hAhXKz4UKHUvWCscQFjAKegQIAxAB&url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F320978810_An_Overview_of_Question_Answering_System&usg=AOvVaw01Y_S9X-3MXMpyrFg8mVqL\n",
    "2. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=13&cad=rja&uact=8&ved=2ahUKEwit9rWHls_hAhXKz4UKHUvWCscQFjAMegQIAhAB&url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3825096%2F&usg=AOvVaw04BMj_m3ELsoau6ylXl6Pn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_QU_pewNEAC"
   },
   "source": [
    "# ParlAI\n",
    "[ParlAI](https://github.com/facebookresearch/ParlAI/blob/master/README.md) (pronounced “par-lay”) is a framework for dialogue AI research, implemented in Python.\n",
    "\n",
    "Its goal is to provide researchers:\n",
    "\n",
    "* a unified framework for sharing, training and testing dialogue models\n",
    "* many popular datasets available all in one place -- from open-domain chitchat to visual question answering.\n",
    "* a wide set of reference models -- from retrieval baselines to Transformers.\n",
    "* seamless integration of Amazon Mechanical Turk for data collection and human evaluation\n",
    "* integration with Facebook Messenger to connect agents with humans in a chat interface\n",
    "\n",
    "Documentation can be found [here](http://www.parl.ai/static/docs/), some of this tutorial is inspired from the ParlAI documentation so feel free to go back and forth between the notebook and the documentation.\n",
    "\n",
    "\n",
    "### Setup the notebook\n",
    "If using google colab, make sure to use TPU runtime by going to ***Runtime > Change runtime type > Hardware accelerator: TPU > Save***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqNqCH3XNOp-"
   },
   "source": [
    "### Install ParlAI\n",
    "\n",
    "Start by installing ParlAI from github. The ParlAI folder will be located in the home directory at `~/ParlAI/`.  \n",
    "*Note: In a jupyter notebook, you can run arbitrary bash commands by prefixing them with a question mark, example: `!echo \"Hello World\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "S0YKXmMUNe_C",
    "outputId": "e71c76bc-dd04-4626-e02c-8e61eb4880fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/root/ParlAI'...\n",
      "remote: Enumerating objects: 114, done.\u001b[K\n",
      "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
      "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
      "remote: Total 20716 (delta 60), reused 75 (delta 38), pack-reused 20602\u001b[K\n",
      "Receiving objects: 100% (20716/20716), 22.03 MiB | 19.00 MiB/s, done.\n",
      "Resolving deltas: 100% (14375/14375), done.\n",
      "zip_safe flag not set; analyzing archive contents...\n"
     ]
    }
   ],
   "source": [
    "# Remove `> /dev/null` to see the output of commands\n",
    "!rm -fr ~/ParlAI\n",
    "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
    "!cd ~/ParlAI; python setup.py develop > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A15LLyvRNU65"
   },
   "source": [
    "Most of the scripts that we will use in ParlAI are located in the ~/ParlAI/examples directory.\n",
    "Let's have a first glance at the scripts available, we will come back to them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "8xLsQcefNZr8",
    "outputId": "0cb957d2-d5e3-4c43-d75d-472f40f56fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_train.py\t       eval_model.py\t\t remote.py\n",
      "build_dict.py\t       extract_image_feature.py  seq2seq_train_babi.py\n",
      "build_pytorch_data.py  interactive.py\t\t train_model.py\n",
      "display_data.py        profile_train.py\n",
      "display_model.py       README.md\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ParlAI/examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8BCBD6vOY6V"
   },
   "source": [
    "## 1. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stay4iHqOcVq"
   },
   "source": [
    "**First** we need to download the data, we will use the `build_dict.py` as a dummy task to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "9c2i-fGaOQOv",
    "outputId": "df5c53c0-3d0c-4b49-c326-b2384c4563da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading babi.tar.gz: 100% 19.2M/19.2M [00:01<00:00, 13.3MB/s]\n",
      "Building dictionary: 100% 900/900 [00:00<00:00, 20.5kex/s]\n",
      "1 Mary moved to the bathroom.\n",
      "2 John went to the hallway.\n",
      "3 Where is Mary? \tbathroom\n",
      "4 Daniel went back to the hallway.\n",
      "5 Sandra moved to the garden.\n",
      "6 Where is Daniel? \thallway\n",
      "7 John moved to the office.\n",
      "8 Sandra journeyed to the bathroom.\n",
      "9 Where is Daniel? \thallway\n",
      "10 Mary moved to the hallway.\n",
      "11 Daniel travelled to the office.\n",
      "12 Where is Daniel? \toffice\n",
      "13 John went back to the garden.\n",
      "14 John moved to the bedroom.\n",
      "15 Where is Sandra? \tbathroom\n",
      "1 Mary went to the bedroom.\n",
      "2 John journeyed to the bathroom.\n",
      "3 Where is John? \tbathroom\n",
      "4 Sandra journeyed to the hallway.\n",
      "5 John journeyed to the garden.\n",
      "6 Where is Mary? \tbedroom\n",
      "7 John journeyed to the bathroom.\n",
      "8 Sandra journeyed to the garden.\n",
      "9 Where is John? \tbathroom\n",
      "10 Sandra went back to the bedroom.\n",
      "11 Daniel travelled to the bathroom.\n",
      "12 Where is John? \tbathroom\n",
      "13 John went to the office.\n",
      "14 Mary moved to the office.\n",
      "15 Where is Sandra? \tbedroom\n"
     ]
    }
   ],
   "source": [
    "# Download the data silently\n",
    "!python ~/ParlAI/examples/build_dict.py --task babi:task1k:1 --dict-file /tmp/babi1.dict > /dev/null\n",
    "# Print a few examples\n",
    "!head -n 30 ~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apxdE1h0OoKs"
   },
   "source": [
    "The bAbI tasks were downloaded in `~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/`\n",
    "\n",
    "In bAbI the data is organised as follows:\n",
    "- **Dialog turn**: A dialog turn is a single utterance / statement. Each line in the file corresponds to one dialog turn.   \n",
    "  Example: *\"John went to the office.\"*\n",
    "- **Sample (question)**: Every few dialog turns, a question can be asked that the model has to answer, this consitute a sample.  The question is followed by its ground truth answer, separated by a tab.\n",
    "  Example: *\"Where is John? `<tab>` bathroom\"*\n",
    "- **Episode**: a sequence of ordered coherent dialog turns that are related to each other form an episode. Each new episode is independant of the others. Each line starts with the dialog turn number in the current episode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-XeHnQIO_3c"
   },
   "source": [
    "## 3. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgge2s_2O9z0"
   },
   "source": [
    "We now have a clearer idea of the data distribution and the metrics that we can use.  \n",
    "The next step is to start solving the tasks with a simple baseline. This will allow us to compare more elaborate models agains this baseline.  \n",
    "Here are a few classical baselines:\n",
    "- **Random model**: The model answers randomly among the set of possible answers for each question\n",
    "-  **Majority class**: The model always answers with the most frequent answer in the training set (majority class)\n",
    "\n",
    "We are going to reimplement these own baselines.  \n",
    "Implementing a new model in ParlAI is detailed in the [tutorial](http://parl.ai/static/docs/seq2seq_tutorial.html) but for our simple baselines, we will only need to inherit the [Agent](https://github.com/facebookresearch/ParlAI/blob/6d246842d3f4e941dd3806f3d9fa62f607d48f59/parlai/core/agents.py#L50) class and override the `act()` method.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Note: the `%%writefile` magic command in jupyter writes the content of the cell to a file at the given path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJJQF0MwOwW9"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/ParlAI/parlai/agents/baseline/  ## create a folder to store our baseline files\n",
    "!touch ~/ParlAI/parlai/agents/baseline/random.py ## create the random baseline file\n",
    "!touch ~/ParlAI/parlai/agents/baseline/majorityclass.py ## create the majority baseline file\n",
    "\n",
    "!mkdir -p ~/ParlAI/parlai/agents/rnn_model/    ## create a folder to store rnn model\n",
    "!touch ~/ParlAI/parlai/agents/rnn_model/rnn_model.py ## create rnn model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aT1k7cmVQvO7",
    "outputId": "5ae759e3-cad9-4285-f86e-f9c43af88764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/baseline/random.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/ParlAI/parlai/agents/baseline/random.py\n",
    "import random\n",
    "\n",
    "from parlai.core.torch_agent import Agent   ## import the Agent model where our baseline will inherit\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "  \n",
    "    def act(self):\n",
    "        if 'label_candidates' not in self.observation:   ## check if the attribute label_candidates is part of oservation if not we will not do anything\n",
    "            return\n",
    "        candidates = list(self.observation['label_candidates'])  ## the candidates answer\n",
    "        reply = {'text': candidates[random.randrange(len(candidates))]} ## choose randomly one of the candidate answer\n",
    "        return reply  ## and return the choosen answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VfGjpkISRCRC",
    "outputId": "5f69f7b2-f274-415a-efc8-c8434743e22f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exs': 1000, 'accuracy': 0.173, 'f1': 0.173, 'bleu': 1.73e-10}\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/random | grep accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1853
    },
    "colab_type": "code",
    "id": "k7PfK0lJRHIT",
    "outputId": "17c33e7e-01e2-49a9-e34f-e5207a4cf305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: baseline/random ]\n",
      "[  model_file: None ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "\n",
      "********************************************************************************\n",
      "Thank you for using ParlAI! We are conducting a user survey.\n",
      "Please consider filling it out at https://forms.gle/uEFbYGP7w6hiuGQT9\n",
      "********************************************************************************\n",
      "\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   office\n",
      "~~\n",
      "[babi:task10k:1]: John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   office\n",
      "~~\n",
      "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[eval_labels: hallway]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   kitchen\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[eval_labels: garden]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|bedroom|kitchen|garden|hallway|...and 1 more]\n",
      "   office\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/random -n 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "485qcDw_RNrx",
    "outputId": "75837914-84a3-4486-d9ca-6d172fb39c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/baseline/majorityclass.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/ParlAI/parlai/agents/baseline/majorityclass.py\n",
    "import random\n",
    "\n",
    "from parlai.core.torch_agent import Agent\n",
    "\n",
    "\n",
    "class MajorityclassAgent(Agent):\n",
    "  \n",
    "    def act(self):\n",
    "        # From the previous answers:\n",
    "        # Possible answers: Counter({'bathroom': 1564, 'hallway': 1517, 'garden': 1508, 'bedroom': 1473, 'kitchen': 1471, 'office': 1467}) (6)\n",
    "        # So the most common answer is 'bathroom'\n",
    "        reply = {'text': 'bathroom'}\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "act6OmTGRZEm",
    "outputId": "3a960823-df91-4ca8-8b71-9a9daf2b5249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/majorityclass | grep accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1853
    },
    "colab_type": "code",
    "id": "klWsVvXIRaub",
    "outputId": "39dccd93-9d56-49b8-ca39-80b42459603a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: baseline/majorityclass ]\n",
      "[  model_file: None ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "\n",
      "********************************************************************************\n",
      "Thank you for using ParlAI! We are conducting a user survey.\n",
      "Please consider filling it out at https://forms.gle/uEFbYGP7w6hiuGQT9\n",
      "********************************************************************************\n",
      "\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[eval_labels: hallway]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[eval_labels: garden]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|office|garden|hallway|bathroom|...and 1 more]\n",
      "   bathroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/majorityclass -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbJkZ5gR0IzD"
   },
   "source": [
    "# 4 : Our RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73ZNU0_a0VEW"
   },
   "source": [
    "Now it is time to create and train our very own model for Question Answering !\n",
    "\n",
    "In order to to so, we will create a class called **RnnModelAgent** that will represent our model in ParlAI and will use a GRU to make the predictions.\n",
    "\n",
    "Our GRU model is made of the following:\n",
    "\n",
    "1) An **Embedding layer** : it is simply a lookup table that given a token index (for example 24 for the word \"bathroom\") will return the corresponding word vector of dimension *hidden_size*. The number of rows in this layer will be equal to the number of words in our dictionnary (since for each of them it has to provide a dense representation : the word  vector)\n",
    "\n",
    "2) A **GRU layer** as explained in Section 0\n",
    "\n",
    "3) A **Decoder** that will transform the output of the GRU into a vector of score for each word of the dictionnary. The higher the score for a word, the most likely this word is the correct answer to the question (according to the model)\n",
    "\n",
    "A ParlAI agent has to implement 3 main methods:\n",
    "\n",
    "1) **__init__**  where we will initialize our model, define useful constants and create the optimizer\n",
    "\n",
    "2) **train_step** : it is the core of our agent. Given a batch of input (stories, questions and answers), we will write the code that compute the model's prediction, the loss of this prediction and apply the backpropagation algorithm on the loss using the optimizer\n",
    "\n",
    "3) **eval_step** : Given a batch of stories and questions (notice that answers are not provided since it is the *evaluation* phase), we will write the code that will output the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "I-0gkt_TRcDl",
    "outputId": "c97ead1d-bc9f-4723-f78e-080d7b2378f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/rnn_model/rnn_model.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile ~/ParlAI/parlai/agents/rnn_model/rnn_model.py\n",
    "##import parlai and pytorch\n",
    "from parlai.core.torch_agent import TorchAgent, Output\n",
    "from parlai.core.utils import padded_3d\n",
    "from parlai.core.logs import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "losses = []\n",
    "##define the model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size ## input of the model: number of characters in our dictionary\n",
    "        self.hidden_size = hidden_size ## number of neurons per layers\n",
    "        self.output_size = output_size ## output size of the model: number of charaters in the output\n",
    "        self.n_layers = n_layers  ## number of layers of the model\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)## map a characters to a fixed size of vector (for characters embeddings)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers) ## define the GRU part:it takes asinput the embedding vectors from the encoder\n",
    "        self.decoder = nn.Linear(hidden_size, output_size) ## decode the vectors output of the GRU to characters \n",
    "    \n",
    "    def forward(self, input, hidden): \n",
    "        input = self.encoder(input)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.decoder(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=32):\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)) ##initialize all the hidden state to zero\n",
    "\n",
    "class RnnModelAgent(TorchAgent):\n",
    "    \n",
    "    @staticmethod    \n",
    "    def add_cmdline_args(argparser):  ## to allow to run the model on command line\n",
    "        agent = argparser.add_argument_group(\"Simple RNN Arguments\")\n",
    "        \n",
    "        agent.add_argument(\"-hs\", \"--hidden-size\", type=int, default=128, help=\"Size of the hidden layer(s) of the RNN\")\n",
    "        agent.add_argument(\"-nl\", \"--num-layers\", type=int, default=1, help=\"Number of layers of the RNN\")\n",
    "        \n",
    "        TorchAgent.add_cmdline_args(argparser)\n",
    "        RnnModelAgent.dictionary_class().add_cmdline_args(argparser)\n",
    "        \n",
    "        return agent\n",
    "    \n",
    "    def __init__(self, opt, shared=None):\n",
    "        \n",
    "        super().__init__(opt, shared)\n",
    "        \n",
    "        if opt['tensorboard_log'] is True:\n",
    "            self.writer = TensorboardLogger(opt)\n",
    "        \n",
    "        self.dictionnary_size = 26      ## we suppose that our dictionary contain the 26 characters of the alphabet\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "        self.use_cuda = True if torch.cuda.is_available() else False\n",
    "        self.embedding_dim = opt[\"hidden_size\"]  ##the dimension of our embemding vector after transforming the character to a vector\n",
    "        self.n_layers = opt[\"num_layers\"]  ## number of layer of the network\n",
    "        self.batch_size = opt[\"batchsize\"]  ## number of examples that we are going to pass as an input at the same time to the network\n",
    "        self.criterion = nn.NLLLoss()  ### define the loss of our network\n",
    "          \n",
    "        if (shared is not None) and (\"decoder\" in shared):\n",
    "          self.decoder = shared[\"decoder\"]\n",
    "          \n",
    "        else:\n",
    "          self.decoder = RNN(self.dictionnary_size, self.embedding_dim, self.dictionnary_size, self.n_layers) ## This RNN refer to the previous RNN class\n",
    "          \n",
    "          \n",
    "        self.decoder.to(self.device)\n",
    "        self.model = self.decoder\n",
    "        \n",
    "        def weight_init(m):   ##initialisation of the weights of our model using xavier\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "        \n",
    "        self.decoder.apply(weight_init)    #apply the initialized weight to the model\n",
    "        self.optimizer = optim.Adam(self.decoder.parameters()) ## use Adam optimizer for the model to perform backpropagation(weight updating)\n",
    "        self.scheduler = None\n",
    "        self.warmup_scheduler = None\n",
    "        self.batch_iter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def vectorize(self, *args, **kwargs):    ## this function is called by parlai to preprocess text\n",
    "        \"\"\"Override options in vectorize from parent.\"\"\"\n",
    "        kwargs['add_start'] = False       ##to avoid starting at  the start of sentence\n",
    "        kwargs['add_end'] = False\n",
    "        return super().vectorize(*args, **kwargs)\n",
    "      \n",
    "      \n",
    "    ## the next function allows us to train the model we defined so far\n",
    "    def train_step(self, batch):  \n",
    "        \n",
    "        self.decoder.train()\n",
    "        \n",
    "        contexts, answers = batch.text_vec, batch.label_vec # extract the different contexts and answers from the batch input\n",
    "        \n",
    "        self.optimizer.zero_grad() ## Since the backward() function accumulates gradients, and we don’t want to mix up gradients between minibatches,\n",
    "                                   ## we have to zero them out at the start of a new minibatch\n",
    "\n",
    "        loss = 0\n",
    "        hidden = self.decoder.init_hidden(self.batch_size).to(self.device) ## initialize the hidden state to zero by calling the previous init_hidden function\n",
    "\n",
    "        output, hidden = self.decoder(contexts.t(), hidden) \n",
    "        output = output.transpose(0,1)[:,-1,:]              ## to get sequence leng, batch, dimension\n",
    "        loss += self.criterion(output.squeeze(1), answers.squeeze(1)) ## compute the loss by comparing the output of our model and the true answer\n",
    "            \n",
    "        \n",
    "        pred = output.argmax(dim=1)  ## get the predicted answer of our model: it appear at the index which has the bigger value of probbility\n",
    "        \n",
    "        losses.append(loss.item())  ## accumulate the loss\n",
    "         \n",
    "        loss.backward() ## backpropagate the loss to update the parameters of the model\n",
    "        self.optimizer.step()  #specify that we finish with one step optimization \n",
    "        \n",
    "        self.batch_iter+= 1 ## next iteration\n",
    "        \n",
    "        return Output(self.dict.vec2txt(pred).split(\" \")) # convert the vector output to text and return it\n",
    "    \n",
    "    def eval_step(self, batch): #after training the model we need to evaluate it in order to measure its performance\n",
    "      \n",
    "        self.decoder.eval()\n",
    "        \n",
    "        contexts = batch.text_vec ## in the evaluation case we don't have acces to an asnwer, the only thing we have is the question\n",
    "        \n",
    "        hidden = self.decoder.init_hidden(contexts.shape[0]).to(self.device) ## as usually we initialize the hidden state\n",
    "\n",
    "        output, hidden = self.decoder(contexts.t(), hidden)  # compute the output of the model on unseen data\n",
    "        output = output.transpose(0,1)[:,-1,:]    ## to get sequence leng, batch, dimension\n",
    "            \n",
    "        \n",
    "        pred = output.argmax(dim=1)  # get the predicte\n",
    "        \n",
    "        return Output(self.dict.vec2txt(pred).split(\" \"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "PJjg2kKiVn-Q",
    "outputId": "207df4b5-937e-484b-d465-32480e9876a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babi1.dict\tmy_rnn_model.dict      my_rnn_model.test\n",
      "babi1.dict.opt\tmy_rnn_model.dict.opt  my_rnn_model.trainstats\n",
      "my_rnn_model\tmy_rnn_model.opt       my_rnn_model.valid\n"
     ]
    }
   ],
   "source": [
    "#!rm -fr /tmp/*\n",
    "!ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "IiNtn4-Ht8s0",
    "outputId": "33f7eed9-3248-48ca-be20-f21dc3726ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 32 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: rnn_model ]\n",
      "[  model_file: /tmp/my_rnn_model ]\n",
      "[ Training Loop Arguments: ] \n",
      "[  dict_build_first: True ]\n",
      "[  display_examples: False ]\n",
      "[  eval_batchsize: None ]\n",
      "[  evaltask: None ]\n",
      "[  load_from_checkpoint: False ]\n",
      "[  max_train_time: -1 ]\n",
      "[  num_epochs: 10.0 ]\n",
      "[  save_after_valid: False ]\n",
      "[  save_every_n_secs: -1 ]\n",
      "[  short_final_eval: False ]\n",
      "[  validation_cutoff: 1.0 ]\n",
      "[  validation_every_n_epochs: -1 ]\n",
      "[  validation_every_n_secs: -1 ]\n",
      "[  validation_max_exs: -1 ]\n",
      "[  validation_metric: accuracy ]\n",
      "[  validation_metric_mode: None ]\n",
      "[  validation_patience: 10 ]\n",
      "[  validation_share_agent: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_comment:  ]\n",
      "[  tensorboard_log: False ]\n",
      "[  tensorboard_metrics: None ]\n",
      "[  tensorboard_tag: None ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Dictionary Loop Arguments: ] \n",
      "[  dict_include_test: False ]\n",
      "[  dict_include_valid: False ]\n",
      "[  dict_maxexs: -1 ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Simple RNN Arguments: ] \n",
      "[  hidden_size: 128 ]\n",
      "[  num_layers: 1 ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: /tmp/babi1.dict ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "\n",
      "********************************************************************************\n",
      "Thank you for using ParlAI! We are conducting a user survey.\n",
      "Please consider filling it out at https://forms.gle/uEFbYGP7w6hiuGQT9\n",
      "********************************************************************************\n",
      "\n",
      "[ building dictionary first... ]\n",
      "[ dictionary already built .]\n",
      "[ no model with opt yet at: /tmp/my_rnn_model(.opt) ]\n",
      "Dictionary: loading dictionary from /tmp/babi1.dict\n",
      "[ num words =  26 ]\n",
      "[ Using CUDA ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[ training... ]\n",
      "[ time:2.0s total_exs:5120 epochs:0.57 time_left:34.0s ] {'exs': 5120, 'accuracy': 0.3873, 'f1': 0.3873, 'bleu': 3.873e-10, 'num_updates': 0}\n",
      "[ time:4.0s total_exs:10336 epochs:1.15 time_left:31.0s ] {'exs': 5216, 'accuracy': 0.4762, 'f1': 0.4762, 'bleu': 4.762e-10, 'num_updates': 0}\n",
      "[ time:6.0s total_exs:15552 epochs:1.73 time_left:29.0s ] {'exs': 5216, 'accuracy': 0.4576, 'f1': 0.4576, 'bleu': 4.576e-10, 'num_updates': 0}\n",
      "[ time:8.0s total_exs:20768 epochs:2.31 time_left:27.0s ] {'exs': 5216, 'accuracy': 0.4398, 'f1': 0.4398, 'bleu': 4.398e-10, 'num_updates': 0}\n",
      "[ time:10.0s total_exs:25952 epochs:2.88 time_left:25.0s ] {'exs': 5184, 'accuracy': 0.4558, 'f1': 0.4558, 'bleu': 4.558e-10, 'num_updates': 0}\n",
      "[ time:12.0s total_exs:31104 epochs:3.46 time_left:23.0s ] {'exs': 5152, 'accuracy': 0.4373, 'f1': 0.4373, 'bleu': 4.373e-10, 'num_updates': 0}\n",
      "[ time:14.0s total_exs:36320 epochs:4.04 time_left:21.0s ] {'exs': 5216, 'accuracy': 0.445, 'f1': 0.445, 'bleu': 4.45e-10, 'num_updates': 0}\n",
      "[ time:16.0s total_exs:41536 epochs:4.62 time_left:19.0s ] {'exs': 5216, 'accuracy': 0.4273, 'f1': 0.4273, 'bleu': 4.273e-10, 'num_updates': 0}\n",
      "[ time:18.0s total_exs:46688 epochs:5.19 time_left:17.0s ] {'exs': 5152, 'accuracy': 0.4346, 'f1': 0.4346, 'bleu': 4.346e-10, 'num_updates': 0}\n",
      "[ time:20.0s total_exs:51840 epochs:5.76 time_left:15.0s ] {'exs': 5152, 'accuracy': 0.4363, 'f1': 0.4363, 'bleu': 4.363e-10, 'num_updates': 0}\n",
      "[ time:22.0s total_exs:56512 epochs:6.28 time_left:14.0s ] {'exs': 4672, 'accuracy': 0.4232, 'f1': 0.4232, 'bleu': 4.232e-10, 'num_updates': 0}\n",
      "[ time:24.0s total_exs:61152 epochs:6.79 time_left:12.0s ] {'exs': 4640, 'accuracy': 0.4224, 'f1': 0.4224, 'bleu': 4.224e-10, 'num_updates': 0}\n",
      "[ time:26.0s total_exs:65792 epochs:7.31 time_left:10.0s ] {'exs': 4640, 'accuracy': 0.4494, 'f1': 0.4494, 'bleu': 4.494e-10, 'num_updates': 0}\n",
      "[ time:28.0s total_exs:70432 epochs:7.83 time_left:8.0s ] {'exs': 4640, 'accuracy': 0.4179, 'f1': 0.4179, 'bleu': 4.179e-10, 'num_updates': 0}\n",
      "[ time:30.0s total_exs:75072 epochs:8.34 time_left:6.0s ] {'exs': 4640, 'accuracy': 0.4256, 'f1': 0.4256, 'bleu': 4.256e-10, 'num_updates': 0}\n",
      "[ time:32.0s total_exs:80160 epochs:8.91 time_left:4.0s ] {'exs': 5088, 'accuracy': 0.4367, 'f1': 0.4367, 'bleu': 4.367e-10, 'num_updates': 0}\n",
      "[ time:34.0s total_exs:85280 epochs:9.48 time_left:2.0s ] {'exs': 5120, 'accuracy': 0.4406, 'f1': 0.4406, 'bleu': 4.406e-10, 'num_updates': 0}\n",
      "[ time:35.0s total_exs:90016 epochs:10.0 time_left:0s ] {'exs': 4736, 'accuracy': 0.4417, 'f1': 0.4417, 'bleu': 4.417e-10, 'num_updates': 0}\n",
      "[ num_epochs completed:10.0 time elapsed:35.96626567840576s ]\n",
      "Dictionary: saving dictionary to /tmp/my_rnn_model.dict\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[ running eval: valid ]\n",
      "valid:{'exs': 1000, 'accuracy': 0.442, 'f1': 0.442, 'bleu': 4.42e-10, 'num_updates': 0}\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
      "[ running eval: test ]\n",
      "test:{'exs': 1000, 'accuracy': 0.387, 'f1': 0.387, 'bleu': 3.87e-10, 'num_updates': 0}\n"
     ]
    }
   ],
   "source": [
    "!rm -fr /tmp/my_rnn_model*\n",
    "!python ~/ParlAI/examples/train_model.py -t babi:task10k:1 -bs 32 -eps 10 -m rnn_model -mf /tmp/my_rnn_model --dict-file /tmp/babi1.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3203
    },
    "colab_type": "code",
    "id": "RV6KLrxqXMyz",
    "outputId": "a62f7ee1-ec9b-4fdc-f442-671cd06ae37f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 32 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: seq2seq ]\n",
      "[  model_file: /tmp/seq2seq_model ]\n",
      "[ Training Loop Arguments: ] \n",
      "[  dict_build_first: True ]\n",
      "[  display_examples: False ]\n",
      "[  eval_batchsize: None ]\n",
      "[  evaltask: None ]\n",
      "[  load_from_checkpoint: False ]\n",
      "[  max_train_time: -1 ]\n",
      "[  num_epochs: 10.0 ]\n",
      "[  save_after_valid: False ]\n",
      "[  save_every_n_secs: -1 ]\n",
      "[  short_final_eval: False ]\n",
      "[  validation_cutoff: 1.0 ]\n",
      "[  validation_every_n_epochs: -1 ]\n",
      "[  validation_every_n_secs: -1 ]\n",
      "[  validation_max_exs: -1 ]\n",
      "[  validation_metric: accuracy ]\n",
      "[  validation_metric_mode: None ]\n",
      "[  validation_patience: 10 ]\n",
      "[  validation_share_agent: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_comment:  ]\n",
      "[  tensorboard_log: False ]\n",
      "[  tensorboard_metrics: None ]\n",
      "[  tensorboard_tag: None ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Dictionary Loop Arguments: ] \n",
      "[  dict_include_test: False ]\n",
      "[  dict_include_valid: False ]\n",
      "[  dict_maxexs: -1 ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Seq2Seq Arguments: ] \n",
      "[  attention: none ]\n",
      "[  attention_length: 48 ]\n",
      "[  attention_time: post ]\n",
      "[  bidirectional: False ]\n",
      "[  decoder: same ]\n",
      "[  dropout: 0.1 ]\n",
      "[  embeddingsize: 128 ]\n",
      "[  hiddensize: 128 ]\n",
      "[  input_dropout: 0.0 ]\n",
      "[  lookuptable: unique ]\n",
      "[  numlayers: 2 ]\n",
      "[  numsoftmax: 1 ]\n",
      "[  rnn_class: lstm ]\n",
      "[ Torch Generator Agent: ] \n",
      "[  beam_block_ngram: 0 ]\n",
      "[  beam_dot_log: False ]\n",
      "[  beam_min_length: 1 ]\n",
      "[  beam_min_n_best: 3 ]\n",
      "[  beam_size: 1 ]\n",
      "[  skip_generation: False ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: /tmp/babi1.dict ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "\n",
      "********************************************************************************\n",
      "Thank you for using ParlAI! We are conducting a user survey.\n",
      "Please consider filling it out at https://forms.gle/uEFbYGP7w6hiuGQT9\n",
      "********************************************************************************\n",
      "\n",
      "[ building dictionary first... ]\n",
      "[ dictionary already built .]\n",
      "[ no model with opt yet at: /tmp/seq2seq_model(.opt) ]\n",
      "Dictionary: loading dictionary from /tmp/babi1.dict\n",
      "[ num words =  26 ]\n",
      "[ Using CUDA ]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[ training... ]\n",
      "[ time:2.0s total_exs:3680 epochs:0.41 time_left:48.0s ] {'exs': 3680, 'lr': 1, 'num_updates': 115, 'loss': 125.7, 'token_acc': 0.571, 'nll_loss': 1.119, 'ppl': 3.062}\n",
      "[ time:4.0s total_exs:7680 epochs:0.85 time_left:44.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 240, 'loss': 110.7, 'token_acc': 0.6374, 'nll_loss': 0.8854, 'ppl': 2.424}\n",
      "[ time:6.0s total_exs:11680 epochs:1.3 time_left:41.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 365, 'loss': 95.7, 'token_acc': 0.7431, 'nll_loss': 0.7656, 'ppl': 2.15}\n",
      "[ time:8.0s total_exs:15680 epochs:1.74 time_left:39.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 490, 'loss': 92.44, 'token_acc': 0.7461, 'nll_loss': 0.7395, 'ppl': 2.095}\n",
      "[ time:10.0s total_exs:19552 epochs:2.17 time_left:37.0s ] {'exs': 3872, 'lr': 1, 'num_updates': 611, 'loss': 84.97, 'token_acc': 0.7554, 'nll_loss': 0.7022, 'ppl': 2.018}\n",
      "[ time:12.0s total_exs:23584 epochs:2.62 time_left:34.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 737, 'loss': 83.96, 'token_acc': 0.7529, 'nll_loss': 0.6664, 'ppl': 1.947}\n",
      "[ time:14.0s total_exs:27584 epochs:3.06 time_left:32.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 862, 'loss': 80.24, 'token_acc': 0.7601, 'nll_loss': 0.6419, 'ppl': 1.9}\n",
      "[ time:16.0s total_exs:31648 epochs:3.52 time_left:30.0s ] {'exs': 4064, 'lr': 1, 'num_updates': 989, 'loss': 80.49, 'token_acc': 0.7538, 'nll_loss': 0.6338, 'ppl': 1.885}\n",
      "[ time:18.0s total_exs:35648 epochs:3.96 time_left:28.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 1114, 'loss': 78.17, 'token_acc': 0.7479, 'nll_loss': 0.6254, 'ppl': 1.869}\n",
      "[ time:20.0s total_exs:39680 epochs:4.41 time_left:26.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1240, 'loss': 77.6, 'token_acc': 0.7511, 'nll_loss': 0.6158, 'ppl': 1.851}\n",
      "[ time:22.0s total_exs:43744 epochs:4.86 time_left:24.0s ] {'exs': 4064, 'lr': 1, 'num_updates': 1367, 'loss': 76.81, 'token_acc': 0.7538, 'nll_loss': 0.6048, 'ppl': 1.831}\n",
      "[ time:24.0s total_exs:47776 epochs:5.31 time_left:22.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1493, 'loss': 76.04, 'token_acc': 0.7526, 'nll_loss': 0.6035, 'ppl': 1.829}\n",
      "[ time:26.0s total_exs:51712 epochs:5.75 time_left:20.0s ] {'exs': 3936, 'lr': 1, 'num_updates': 1616, 'loss': 74.32, 'token_acc': 0.7495, 'nll_loss': 0.6043, 'ppl': 1.83}\n",
      "[ time:28.0s total_exs:55744 epochs:6.19 time_left:18.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1742, 'loss': 72.61, 'token_acc': 0.7538, 'nll_loss': 0.5762, 'ppl': 1.779}\n",
      "[ time:30.0s total_exs:59776 epochs:6.64 time_left:16.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1868, 'loss': 73.82, 'token_acc': 0.7579, 'nll_loss': 0.5859, 'ppl': 1.797}\n",
      "[ time:32.0s total_exs:63808 epochs:7.09 time_left:14.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 1994, 'loss': 73.01, 'token_acc': 0.7499, 'nll_loss': 0.5795, 'ppl': 1.785}\n",
      "[ time:34.0s total_exs:67840 epochs:7.54 time_left:12.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 2120, 'loss': 71.3, 'token_acc': 0.7578, 'nll_loss': 0.5659, 'ppl': 1.761}\n",
      "[ time:36.0s total_exs:71840 epochs:7.98 time_left:10.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 2245, 'loss': 70.05, 'token_acc': 0.761, 'nll_loss': 0.5604, 'ppl': 1.751}\n",
      "[ time:38.0s total_exs:75872 epochs:8.43 time_left:8.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 2371, 'loss': 71.64, 'token_acc': 0.7545, 'nll_loss': 0.5686, 'ppl': 1.766}\n",
      "[ time:40.0s total_exs:79872 epochs:8.87 time_left:6.0s ] {'exs': 4000, 'lr': 1, 'num_updates': 2496, 'loss': 69.94, 'token_acc': 0.7568, 'nll_loss': 0.5595, 'ppl': 1.75}\n",
      "[ time:42.0s total_exs:83904 epochs:9.32 time_left:4.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 2622, 'loss': 70.62, 'token_acc': 0.7552, 'nll_loss': 0.5605, 'ppl': 1.752}\n",
      "[ time:44.0s total_exs:87936 epochs:9.77 time_left:2.0s ] {'exs': 4032, 'lr': 1, 'num_updates': 2748, 'loss': 71.49, 'token_acc': 0.7505, 'nll_loss': 0.5674, 'ppl': 1.764}\n",
      "[ time:45.0s total_exs:90016 epochs:10.0 time_left:0s ] {'exs': 2080, 'lr': 1, 'num_updates': 2813, 'loss': 35.88, 'token_acc': 0.7587, 'nll_loss': 0.552, 'ppl': 1.737}\n",
      "[ num_epochs completed:10.0 time elapsed:45.21667528152466s ]\n",
      "Dictionary: saving dictionary to /tmp/seq2seq_model.dict\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[ running eval: valid ]\n",
      "valid:{'exs': 1000, 'accuracy': 0.568, 'f1': 0.568, 'bleu': 5.68e-10, 'lr': 1, 'num_updates': 2813, 'loss': 18.46, 'token_acc': 0.784, 'nll_loss': 0.5183, 'ppl': 1.679}\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
      "[ running eval: test ]\n",
      "test:{'exs': 1000, 'accuracy': 0.512, 'f1': 0.512, 'bleu': 5.12e-10, 'lr': 1, 'num_updates': 2813, 'loss': 19.79, 'token_acc': 0.756, 'nll_loss': 0.5661, 'ppl': 1.761}\n"
     ]
    }
   ],
   "source": [
    "!rm -fr /tmp/seq2seq_model*\n",
    "!python ~/ParlAI/examples/train_model.py -t babi:task10k:1 -bs 32 -eps 10 -m seq2seq -mf /tmp/seq2seq_model --dict-file /tmp/babi1.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3797
    },
    "colab_type": "code",
    "id": "8mInOP1lXSd_",
    "outputId": "b99a97ab-2d7e-4228-b892-ad8cba4de8db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  display_ignore_fields: label_candidates,text_candidates ]\n",
      "[  display_prettify: False ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: None ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: None ]\n",
      "[  model_file: /tmp/seq2seq_model ]\n",
      "[ Local Human Arguments: ] \n",
      "[  local_human_candidates_file: None ]\n",
      "[  single_turn: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Seq2Seq Arguments: ] \n",
      "[  attention: none ]\n",
      "[  attention_length: 48 ]\n",
      "[  attention_time: post ]\n",
      "[  bidirectional: False ]\n",
      "[  decoder: same ]\n",
      "[  dropout: 0.1 ]\n",
      "[  embeddingsize: 128 ]\n",
      "[  hiddensize: 128 ]\n",
      "[  input_dropout: 0.0 ]\n",
      "[  lookuptable: unique ]\n",
      "[  numlayers: 2 ]\n",
      "[  numsoftmax: 1 ]\n",
      "[  rnn_class: lstm ]\n",
      "[ Torch Generator Agent: ] \n",
      "[  beam_block_ngram: 0 ]\n",
      "[  beam_dot_log: False ]\n",
      "[  beam_min_length: 1 ]\n",
      "[  beam_min_n_best: 3 ]\n",
      "[  beam_size: 1 ]\n",
      "[  skip_generation: False ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "\n",
      "********************************************************************************\n",
      "Thank you for using ParlAI! We are conducting a user survey.\n",
      "Please consider filling it out at https://forms.gle/uEFbYGP7w6hiuGQT9\n",
      "********************************************************************************\n",
      "\n",
      "Dictionary: loading dictionary from /tmp/seq2seq_model.dict\n",
      "[ num words =  26 ]\n",
      "[ Using CUDA ]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[ Loading existing model params from /tmp/seq2seq_model ]\n",
      "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  display_ignore_fields: label_candidates,text_candidates ]\n",
      "[  display_prettify: False ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 32 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: seq2seq ]\n",
      "[  model_file: /tmp/seq2seq_model ]\n",
      "[ Local Human Arguments: ] \n",
      "[  local_human_candidates_file: None ]\n",
      "[  single_turn: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Seq2Seq Arguments: ] \n",
      "[  attention: none ]\n",
      "[  attention_length: 48 ]\n",
      "[  attention_time: post ]\n",
      "[  bidirectional: False ]\n",
      "[  decoder: same ]\n",
      "[  dropout: 0.1 ]\n",
      "[  embeddingsize: 128 ]\n",
      "[  hiddensize: 128 ]\n",
      "[  input_dropout: 0.0 ]\n",
      "[  lookuptable: unique ]\n",
      "[  numlayers: 2 ]\n",
      "[  numsoftmax: 1 ]\n",
      "[  rnn_class: lstm ]\n",
      "[ Torch Generator Agent: ] \n",
      "[  beam_block_ngram: 0 ]\n",
      "[  beam_dot_log: False ]\n",
      "[  beam_min_length: 1 ]\n",
      "[  beam_min_n_best: 3 ]\n",
      "[  beam_size: 1 ]\n",
      "[  skip_generation: False ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: [0.9, 0.999] ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: [0.7] ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: /tmp/seq2seq_model.dict ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "Enter Your Message: Bob is blue\\n What is Bob ?\n",
      "[Seq2Seq]: garden\n",
      "Enter Your Message: Traceback (most recent call last):\n",
      "  File \"/root/ParlAI/examples/interactive.py\", line 18, in <module>\n",
      "    interactive(opt, print_parser=parser)\n",
      "  File \"/root/ParlAI/parlai/scripts/interactive.py\", line 64, in interactive\n",
      "    world.parley()\n",
      "  File \"/root/ParlAI/parlai/core/worlds.py\", line 257, in parley\n",
      "    acts[0] = agents[0].act()\n",
      "  File \"/root/ParlAI/parlai/agents/local_human/local_human.py\", line 39, in act\n",
      "    reply_text = input(\"Enter Your Message: \")\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/interactive.py -mf /tmp/seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tvRZB5bC4-a"
   },
   "source": [
    "# Part 2 (Optional) : Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0A0fpV4bs4o"
   },
   "source": [
    "Language is a method of human communication, either spoken or written, consisting of the use of words in a structured and conventional way. In Natural Language Processing, the set of words used is represented using text. In this section we will define what we understand by text, why language modelling (LM) is useful and give some applications of LM :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKUwTHPOb__i"
   },
   "source": [
    "**A text** is a sequence of token. Tokens can be words, characters or group of characters.\n",
    "\n",
    "\n",
    " A word is a meaningful sequence of charaters.\n",
    " \n",
    " Example :\n",
    "                                      \n",
    "    {INDABAX RWANDA} = {IN, DA, BAX,  RWAN, DA}\n",
    " \n",
    "                     = {INDABAX,  , RWANDA}\n",
    "                                              \n",
    "                     ={I, N, D, A, B, A, X,  , R, W, A, N, D, A}\n",
    "  ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLzMC_ThcLsz"
   },
   "source": [
    "**Language modelling**  is a subfield of NLP which goal is to design methods to assign a probability to a text (sequence of tokens) .\n",
    "To illiustrate it , suppose you have a sequence of tokens $(x_1, x_2, x_3, ..., x_n)$, language model estimate probability of this tokens.\n",
    "\n",
    "$$ P(x_1, x_2, x_3, ..., x_n) $$\n",
    "\n",
    "This probability depends of a **vocabulary** which is a set of unique tokens.\n",
    "\n",
    "Let's take an example:\n",
    "We want to determine a probability of a sentence. We can tokenize it as  sequence of words $(x_1, x_2, x_3, ..., x_t)$\n",
    "We can calculate the probability by using the \"Chain rule of Probability\":\n",
    "\n",
    "$ P(x_1, x_2, x_3, ..., x_r)= \\prod_{t=1} ^{r} P(x_t|x_{t-1}, x_{t-2}, ..., x_{1})  $\n",
    "\n",
    "There exists many ways to coumpute probability,\n",
    "it can be done by counting the number of times a sequence of tokens occurs in the dataset, this method is called **Count based language model**\n",
    "\n",
    "Example of dataset:\n",
    "\n",
    "1: 'I am very happy to assist to INDABAX RWANDA '\n",
    "2: ' I am at Kigali Rwanda' \n",
    "3: 'I am really enjoy this summer school'\n",
    "$P( \\text{Rwanda} | \\text{I am at Kigali } ) = \\frac{c(\\text{I am at Kigali } )}{c( \\text{RWANDA})} =  1/2$\n",
    "\n",
    "One limitation of this method appears when we want to model long sentences. In general, the longer the sentence, the lower its probability : it is more likely to see a sequence of tokens like\n",
    "\n",
    "A:  \"I am at Kigali\" in a dataset than\n",
    "\n",
    "B:  \" am at Kigali eating an ice cream at an Italian restauranIt called Sole Lunda somewhere in Remera because I love italian ice cream\". \n",
    "\n",
    "With respect to our previously defined dataset, sequence B is unkwown (since it doesn't appear in the dataset) and the conditional probabilities are as well unknown (We cannot estimate the probability of \"Luna\" given \"\"I am at Kigali eating an ice cream at an Italian restauranIt called Sole\" based on our dataset)\n",
    "\n",
    "To fix  this issue we truncate past to a fixed size window of size $n$ i.e.:\n",
    "\n",
    "$ P(x_1, x_2, x_3, ..., x_r)= \\prod_{t=1} ^{r} P(x_t|x_{t-1}, x_{t-2}, ..., x_{1})  \\approx \\prod_{t=1} ^{r} P(x_t|x_{t-1}, x_{t-2}, ..., x_{t-n})$\n",
    "\n",
    "this method is call **n-gram model**\n",
    "\n",
    "As we said earlier, ** \"n” ** refers to the size of past\n",
    "\n",
    "- Examples:\n",
    "   - Unigram:\n",
    "   \n",
    "       $ P(w_1 , . . . , w_T ) =\\prod_{t=1} ^{T}  P(w_t )$\n",
    "   - Bigram:\n",
    "   \n",
    "     $ P(w_1 , . . . , w_T ) =\\prod_{t=1} ^{T}  P(w_t | w_{t−1} )$\n",
    "   - Trigram:\n",
    "   \n",
    "   $   P(w_1 , . . . , w_T ) =\\prod_{t=1} ^{T}  P(w_t | w_{t−1},  w_{t−2})$\n",
    "   \n",
    "n-grams can be “good enough” in some cases but n-grams cannot capture long term dependencies required\n",
    "to truely model language.\n",
    "Various methods have been developed to overcume this issues. Among them we can **Backoff** and **Interpolation**\n",
    "\n",
    "[more info](http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures/smoothing+backoff.pdf)\n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfvqCRn8cRNK"
   },
   "source": [
    "**Application**\n",
    "\n",
    "- Speech recognition \n",
    "\n",
    "It is  a subfield of Machine Learning where we want to give the ability to a machine to convert spoken language into text. In real life, some sentences may have the same pronunciation : For example, \"Chocolate, I scream\" and \"Chocolate ice scream\" or \"\"Eye eight uh Jerry\" and \"”I ate a cherry\"\". As humans, in a real conversation, we know that the first sentences are more plausible than the seconds:\n",
    "\n",
    " 1) P(”Chocolate, I scream”) < P(”Chocolate ice cream”)\n",
    "  \n",
    " 2) P(”Eye eight uh Jerry\") < P(”I ate a cherry\")\n",
    "  \n",
    "  A machine can acquire this kind of commonsense using an accurate language model that will give it the probabilities of both sentences in order to choose the most likely one\n",
    "- Machine translation\n",
    "\n",
    " It is the process by which a computer software is used to translate a text from one natural language (such as English) to target language (such as French, Kinyrwanda)\n",
    "[more info](https://www.aclweb.org/anthology/D07-1090.pdf). Given a sentence in the source language, we want to know the most probable sentence in a target language such that they have the same meaning. However, multiple sentences in the target language can be grammaticaly correct and have approximately the same meaning as the one in the source language. For example\n",
    "\n",
    "In French \"Jane visite l'Afrique en Septembre\"\n",
    "\n",
    "In English:\n",
    "\n",
    "1) Jane is visiting Africa in September.\n",
    "\n",
    "2) Jane is going to be visiting Africa in September.\n",
    "\n",
    "3) In September, Jane will visit Africa.\n",
    "\n",
    "4) Her African friend welcomed Jane in September.\n",
    "\n",
    "How can a computer choose the correct one ? By using a language model ! As good english speakers, we know that the first translation is more likely than the forth one in this case. A good language model will give the same result\n",
    "  \n",
    "- Optical Character Recognition & Handwriting recognition\n",
    "\n",
    "Optical character recognition is the recognition of printed or written text characters by a computer. This involves photoscanning of the text character-by-character, analysis of the scanned-in image, and then translation of the character image into character codes, such as ASCII, commonly used in data processing.\n",
    "[more info](https://www.youtube.com/watch?v=ZNrteLp_SvY)\n",
    "\n",
    "Again, language models are used to disentangle ambiguities : for example, sometimes it's difficult to see the difference between 0 (zero) and O (the letter) on a handwritten document. A computer can easily confuse \"m0ve fast\" and \"move fast\" and humans too! But as humans we know that the second one is more likely than the first one : \n",
    "\n",
    "  P(”m0ve fast”) < P(”move fast”)\n",
    "  \n",
    " A good language model will give the ability to the computer to output the right sentence\n",
    " \n",
    "- Context sensitive spelling correction\n",
    "\n",
    "It is the task of correcting spelling errors which results in valid words. Our smartphones heavily use language models to do so.\n",
    "\n",
    "Example :\n",
    "\n",
    "  \"Their are problems wit this sentence\" -> \"There are problems with this sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fks0gKntqaTu"
   },
   "source": [
    "**Deep learning for Language Modelling**\n",
    "\n",
    "We know that Deep Learning can be very successful for classification and probability estimation. And as we saw earlier, Language modelling is just conditional probability estimation ! Instead of using n-grams and co., can we replace it by a neural network that will take as input the sequence  $x_{t-1}, x_{t-2}, ..., x_{t-n}$ and output $P(x_t|x_{t-1}, x_{t-2}, ..., x_{t-n})$ ? Yes! And fortunately we have a lot of texts available on Internet i.e. a lot of training data !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0D2fevW3dPu"
   },
   "source": [
    "## Implementation of the RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9aseASAC3kF9"
   },
   "source": [
    "Our goal here is to produce a language model based on Shakespear books : after the training, our model is expect to produce sequences of characters that have a similar style as Shakespear work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "OtgM5hUPYffX",
    "outputId": "815f890b-5ffc-41bf-c90b-ce5ab707cd40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-27 09:01:49--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
      "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
      "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4573338 (4.4M) [text/plain]\n",
      "Saving to: ‘shakespeare_input.txt’\n",
      "\n",
      "shakespeare_input.t 100%[===================>]   4.36M  12.8MB/s    in 0.3s    \n",
      "\n",
      "2019-04-27 09:01:50 (12.8 MB/s) - ‘shakespeare_input.txt’ saved [4573338/4573338]\n",
      "\n",
      "sample_data  shakespeare_input.txt\n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "QadAOrUVcqb3",
    "outputId": "f3346e28-26ed-4a75-a53a-528ca589fa36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 9.1MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.0.23\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nriuehiS3x0N"
   },
   "source": [
    "We start by reading the input file and define the set of valid characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "zOd_X6HEY9HC",
    "outputId": "72b4c2d2-4419-410e-a9e6-c82598a70a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "n_characters :  100\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('shakespeare_input.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "print(\"n_characters : \", n_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfJPEFCx36mL"
   },
   "source": [
    "Since we will do mini-batch training, we have to create a function that will extract a \"batch\" i.e. a piece of text from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "mRdLhL08ZYCI",
    "outputId": "4c3d34a5-7064-4275-c794-ed745db2b584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is his sword;\n",
      "I robb'd his wound of it; behold it stain'd\n",
      "With his most noble blood.\n",
      "\n",
      "OCTAVIUS CAESAR:\n",
      "Look you sad, friends?\n",
      "The gods rebuke me, but it is tidings\n",
      "To wash the eyes of kings.\n",
      "\n",
      "AGRIPPA:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnBTeNau4HCE"
   },
   "source": [
    "Then it is time to create our model : we will use the exact same model as in the previous section (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlUsTGXvZZhm"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6NGHESg4M-m"
   },
   "source": [
    "We know that our model only understand numbers. We therefore have to convert each character to a fixed number or more generally, a sequence of characters (a string) into a list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fXoDhfcUdZof",
    "outputId": "310e704b-a7dc-46ec-8248-64c14f5b0471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWoAtSFr4bRU"
   },
   "source": [
    "Based on random_chunk, we have to write a function that will extract a mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm2HMLCdddmL"
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoI9_ROX4miq"
   },
   "source": [
    "This is a code spinnet to evaluate our model assuming that the first characters are \"*prime_str*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_P2iFx7diGe"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden().to(device)\n",
    "    prime_input = char_tensor(prime_str).to(device)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1].to(device)    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char).to(device)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTGoOzDfAmKW"
   },
   "source": [
    "Standard code to measure durations (it will be useful for the training phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mf3Z7Nrbdl_u"
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mreUyFGdAw54"
   },
   "source": [
    "A helper function that will handle the actual training of a mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ehw695MdohO"
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden().to(device)\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].reshape(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2Wgqfd6BNaZ"
   },
   "source": [
    "The entire code for training : here, we define useful constants, the model, do some logging on the loss etc... And call the \"train\" function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2086
    },
    "colab_type": "code",
    "id": "VloBXyH0d54s",
    "outputId": "99222407-5bd9-462a-ecde-7486ec86036a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 28s (100 1%) 2.8011]\n",
      "Wharme to dso seonh us.\n",
      "\n",
      "CDETDNI:\n",
      "Los pepe by it piles bhrat shekerer mome sos an, tuo this coed, in.\n",
      " \n",
      "\n",
      "[0m 55s (200 2%) 2.2567]\n",
      "Whher.\n",
      "Sousere me qunshend I bounh roth hrSerener withing ungoriill.\n",
      "I mons the doir holtind he\n",
      "werle  \n",
      "\n",
      "[1m 24s (300 3%) 2.1984]\n",
      "Whot were to shin's bens.\n",
      "\n",
      "AURET:\n",
      "Hliss, to the were sath the mund be rot:\n",
      "And walpe he go of losk mos \n",
      "\n",
      "[1m 51s (400 4%) 2.1606]\n",
      "Whaor, and bove sall whiting thas pall swrasd do ins\n",
      "The dand you eans rom band wovounterned a worde!  \n",
      "\n",
      "[2m 19s (500 5%) 2.1123]\n",
      "Why hese pirllim,\n",
      "Tone, is in sith comainters ight am them am,\n",
      "Fall, me carll my moleerfortoulliomor!\n",
      " \n",
      "\n",
      "[2m 47s (600 6%) 2.1412]\n",
      "Where, aysief.\n",
      "\n",
      "FOOONBUSSES:\n",
      "Nom ot shich own; I sith withing were plaguts-tall on woulg if stey a han \n",
      "\n",
      "[3m 15s (700 7%) 2.1021]\n",
      "Wh he thou nogh well.\n",
      "\n",
      "FUKIRRTILHEGT:\n",
      "Gemy cad, wide to he fear;\n",
      "To 'bast, a' the for thy be for.\n",
      "\n",
      "VAL \n",
      "\n",
      "[3m 42s (800 8%) 2.3949]\n",
      "Wh wall en mpais'd;\n",
      "Bage heik and a contions his lead;\n",
      "To her prome, you relfell of think strepnedsed, \n",
      "\n",
      "[4m 11s (900 9%) 1.9368]\n",
      "Whis the tome what the the tind whier\n",
      "I'll though the time and our comand his mictest,\n",
      "Hight uncalles  \n",
      "\n",
      "[4m 38s (1000 10%) 1.9809]\n",
      "Whiencae, sett so be and his me sonet sow;\n",
      "I there speok he cofts in thild: then it the fowns w. ale,  \n",
      "\n",
      "[5m 5s (1100 11%) 2.1727]\n",
      "Whed, and the lement starpter:\n",
      "I will you flay I rastien: for burs, I stemse\n",
      "the vasters to livies itr \n",
      "\n",
      "[5m 35s (1200 12%) 2.2548]\n",
      "Wheldales is of thee good for a their reice us sould,\n",
      "And my loeth impentrow,\n",
      "So me of theare not as\n",
      "W \n",
      "\n",
      "[6m 2s (1300 13%) 2.1556]\n",
      "Whis arge the compice of but thou fortest not the fore hast of of the fir for to be timine distent to  \n",
      "\n",
      "[6m 30s (1400 14%) 1.9176]\n",
      "Whim man to recuman fat the oth.\n",
      "\n",
      "LUUCLEM:\n",
      "Or the knere this moted all ther of they the lord?\n",
      "\n",
      "PURE:\n",
      "\n",
      " \n",
      "\n",
      "[6m 57s (1500 15%) 1.9237]\n",
      "What best this acom to the wiplest.\n",
      "\n",
      "VOMIVA:\n",
      "Bevore, I was shaw indon if porve age moing would cartter \n",
      "\n",
      "[7m 25s (1600 16%) 1.8127]\n",
      "Whis you the mens:\n",
      "These dod he seet the hove. I'll! I is as from, he the pade:\n",
      "This go not fenpon enq \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-882517906776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a62e78e79030>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inp, target)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 128\n",
    "n_layers = 3\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder.to(device)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  inputs, labels = random_training_set()\n",
    "  inputs, labels = inputs.to(device), labels.to(device)\n",
    "  loss = train(inputs, labels)       \n",
    "  loss_avg += loss\n",
    "\n",
    "  if epoch % print_every == 0:\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "    print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "  if epoch % plot_every == 0:\n",
    "    all_losses.append(loss_avg / plot_every)\n",
    "    loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "ZAqSa7Hlewab",
    "outputId": "699f5f82-68d9-4e08-bbeb-b4d2604ae26a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdbd81cea90>]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmnw4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "hW7VKVTdyu7s",
    "outputId": "e049ab9c-7109-45eb-bddf-4bfef14d165b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thitherou heath serses of thou adere latch can, besest of the klow nother,\n",
      "This his a stayy we benent henbant her him Movence, gear the litstled,\n",
      "The stist he way, in like and lord of the seet faistrang\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ldSZxXzN-6jc",
    "outputId": "64d6042f-f36a-48f0-b467-c38cbd4432a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This he who whose the best she with the should have for of thee hear the words for of hear of the sheld hear the words,\n",
      "That have she with of the should have have the have have the looks of the her best\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "CvoXJRL0_A-i",
    "outputId": "9cf0140f-7c47-4798-ec2b-9a2be9371f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thas ows rowzes, ullorthelitgry, in befootiounsle yhen, cour, neverp otch: mary henaslesh,\n",
      "BRYANSY EClINThO:\n",
      "His wE ons efflighmaneds? that! meglie?\n",
      "\n",
      "TLcANY:\n",
      "\n",
      "CIPTUR:\n",
      "IN CSlood: measow CnArs; bust\n",
      "Secoh\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6F8Y0zEq-MCL"
   },
   "source": [
    "## Other funny datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-E-TO-J-UAJ"
   },
   "source": [
    "**Linux source code **: you can find a sample here https://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt\n",
    "Or gather the entire source code into one file using the following commands:\n",
    "\n",
    " git clone https://github.com/torvalds/linux.git\n",
    " \n",
    " cd linux\n",
    " \n",
    " find . -name \"*.[c|h]\" | shuf | xargs cat > linux.txt\n",
    "\n",
    "**Algebra in latex** : https://github.com/stacks/stacks-project\n",
    "\n",
    "**Obama speeches** : https://github.com/samim23/obama-rnn/blob/master/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKc_Hm-iAVnl"
   },
   "source": [
    "## Further resources\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1nleOy0AX0E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_NLP_IndabaX_Kigali_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
